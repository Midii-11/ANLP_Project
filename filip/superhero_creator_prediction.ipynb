{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superhero Creator Prediction\n",
    "\n",
    "Code adapted from [kaggle](Code taken from https://www.kaggle.com/someadityamandal/superheroes-visualization-and-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the necessary libraries.\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a suite of libraries and programs for symbolic and statistical natural language\n",
    "processing (NLP) for English\n",
    "The `nltk.download('...')` commands only have to be executed once on a system.\n",
    "\n",
    "`tqdm` allows for progress bars in Jupyter notebooks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipschlembach/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/filipschlembach/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Code taken from https://www.kaggle.com/someadityamandal/superheroes-visualization-and-prediction\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                        history_text        creator\n0  Delroy Garrett, Jr. grew up to become a track ...  Marvel Comics",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>history_text</th>\n      <th>creator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Delroy Garrett, Jr. grew up to become a track ...</td>\n      <td>Marvel Comics</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../datasets/superheroes_nlp_dataset.csv\")\n",
    "\n",
    "data_text = data[['history_text', 'creator']]\n",
    "# we will only select comics by Marvel or DC as there's too many comic creators\n",
    "# todo: idea: group all other cretors under one label and see how the performance changes\n",
    "data_text = data_text.loc[data_text['creator'].isin(['Marvel Comics', 'DC Comics'])]\n",
    "data_text.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining modules for Text Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\", \".join(stopwords.words('english'))\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*',\n",
    "          '+', '\\\\', '•', '~', '@', '£',\n",
    "          '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…',\n",
    "          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥',\n",
    "          '▓', '—', '‹', '─',\n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾',\n",
    "          'Ã', '⋅', '‘', '∞',\n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹',\n",
    "          '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    \"\"\"\n",
    "    This method adds whitespaces before and after punctuation.\n",
    "    :param x: text\n",
    "    :return: text with whitespaces before and after punctuation\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    \"\"\"\n",
    "    This method replaces numerals with up to 5 #-symbols.\n",
    "    :param x: text\n",
    "    :return: text with numerals replaced by # symbols\n",
    "    \"\"\"\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # self explanatory\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords_list])\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    This method lemmatizes the words in the given text. It uses a caching object (for performance improvement?)\n",
    "    :param text: in it's original form\n",
    "    :return: text cosisting of lemmatized words\n",
    "    \"\"\"\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "    class FasterStemmer(object):\n",
    "        def __init__(self):\n",
    "            self.words = {}\n",
    "\n",
    "        def stem(self, x):\n",
    "            if x in self.words:\n",
    "                return self.words[x]\n",
    "            t = lemma.lemmatize(x)\n",
    "            self.words[x] = t\n",
    "            return t\n",
    "\n",
    "    faster_stemmer = FasterStemmer()\n",
    "    text = text.split()\n",
    "    stemmed_words = [faster_stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    del faster_stemmer\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fae58e3f915f4f549598c0d6a96ab0b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae3e2487a37042448c867a56ec772baa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "173e897d60aa4c9794f0c36398146089"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af7b96ca3465406985d70c1e7d2faf5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "471e28751b3c42dd82b668fe47c170c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        history_text        creator\n",
      "0  delroy garrett , jr . grew become track star c...  Marvel Comics\n"
     ]
    }
   ],
   "source": [
    "# apply text preprocessing\n",
    "\n",
    "data_text['history_text'] = data_text['history_text'].progress_apply(lambda x: str(x).lower())\n",
    "data_text['history_text'] = data_text['history_text'].progress_apply(lambda x: clean_text(x))\n",
    "data_text['history_text'] = data_text['history_text'].progress_apply(lambda x: clean_numbers(x))\n",
    "data_text['history_text'] = data_text['history_text'].progress_apply(lambda x: remove_stopwords(x))\n",
    "data_text['history_text'] = data_text['history_text'].progress_apply(lambda x: stem_text(x))\n",
    "print(data_text.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a binary label for the target column `y`, i.e. the creator, and a new variable `X` for the input."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = data_text['creator']\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "X = data_text['history_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Splitting the dataset into training and test data.\n",
    "- [ ] Bag of words mystery. How does this work?!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 80-20 splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                    , test_size=0.35, random_state=1234)\n",
    "# defining the bag-of-words transformer on the text-processed corpus\n",
    "bow_transformer = CountVectorizer(analyzer='word').fit(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_train = bow_transformer.transform(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_test = bow_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [ ] what kind of model is used here? How does it work?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score on training data:\n",
      "0.9796511627906976\n",
      "\n",
      "Score on test data:\n",
      "0.8733153638814016\n"
     ]
    }
   ],
   "source": [
    "# ???\n",
    "\n",
    "# instantiating the model with simple Logistic Regression..\n",
    "model = LogisticRegression()\n",
    "# training the model...\n",
    "model = model.fit(text_bow_train, y_train)\n",
    "\n",
    "print('\\nScore on training data:')\n",
    "print(model.score(text_bow_train, y_train))\n",
    "print('\\nScore on test data:')\n",
    "print(model.score(text_bow_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [ ] Concatenate the two text columns and measure the performance difference.\n",
    "Does more data help?\n",
    "How much more data do we actually get by concatenating the two columns?\n",
    "- [ ] Use [Keras Tokennizer](https://www.youtube.com/watch?v=UFtXy0KRxVI) for BoW and try to replicate the result.\n",
    "- [ ] Add TF-IDF part with explanation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}